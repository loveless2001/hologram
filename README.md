# Hologram Memory

A holographic memory sandbox that anchors multi-modal traces to glyphs, stores them in a lightweight vector index, and experiments with "memory gravity" fields to model concept drift.

---

## ‚ú® Latest Features (Nov 2023)

### üî¨ GLiNER-Powered Concept Decomposition
- **Automatic sentence ‚Üí atomic concepts**: Full sentences are decomposed into semantic units using GLiNER (Generalist Named Entity Recognition)
- **Relation extraction**: Captures verbs and actions (e.g., "hit", "describes") to preserve Subject‚ÜíVerb‚ÜíObject flow
- **Order preservation**: Extracted concepts maintain narrative order for better memory reconstruction
- **Labels**: `concept`, `entity`, `phenomenon`, `object`, `theory`, `action`, `relationship`, `interaction`, `verb`

### üîç Semantic Search Interface
- **REST API**: `/search` endpoint for keyword-based semantic search
- **Streamlit UI**: Two-tab interface with Chat and Semantic Search
- **Visual results**: Color-coded similarity scores (üü¢ 80%+, üü° 60-79%, üîµ <60%)
- **Adjustable results**: Query 1-20 top matches

### üß¨ Concept Mitosis (Contextual Disambiguation)
- **Automatic Splitting**: Detects when a concept is under "semantic tension" (pulled by opposing clusters, e.g., "Field" in Physics vs. Agriculture).
- **Soft Mitosis**: Splits the node into `Concept_1` and `Concept_2`, reassigns neighbors, and creates a weak "bridge link" to preserve metaphorical connections.
- **Dynamic Evolution**: The memory graph automatically refines itself as new contexts are introduced.

### üï∏Ô∏è Graph-Based Reconstruction
- **Structured Retrieval**: Instead of a flat list, retrieves a semantic subgraph (nodes + mass + relations).
- **LLM Synthesis**: Prompts the LLM with the structured graph JSON, enabling it to "reason" over the connections and synthesize a coherent narrative from memory shards.

### üìä Concept Visualization
- **D3.js scatter plot**: 2D PCA projection of concept space at `/viz/viz.html`
- **Interactive**: Hover tooltips, zoom/pan, auto-refresh
- **Human-readable labels**: Shows actual text content instead of hash IDs

---

## Highlights
- Glyph anchors (`üùû`, `üúÇ`, `memory:gravity`, ‚Ä¶) collect related traces across sessions.
- Dual encoder stack: hashing fallback works everywhere; OpenCLIP adds semantic text ‚Üî image retrieval.
- **Negation-aware** gravity field powered by FAISS: positive statements attract concepts, negative statements repel them.
- **Reinforcement-based decay**: unreinforced concepts drift to memory periphery and lose influence over time.
- JSON-backed memory store with glyph registry, summarisation helpers, and **optimized state persistence** (saves final vector state to avoid costly replay).
- Chat orchestration (`chat_cli.py`) with session logs plus a tiny FastAPI surface for programmatic access.

---

## Components

### Core Package (`hologram/`)
- `api.py` ‚Äì public `Hologram` API (`add_text`, `add_image_path`, search, persistence)
- `chatbot.py` ‚Äì chat memory, provider abstractions, CLI orchestration helpers
- `gravity.py` ‚Äì concept drift simulation and FAISS-backed `GravityField` (supports state export/import)
- `embeddings.py` ‚Äì hashing encoders and CLIP wrappers (text and image)
- **`text_utils.py`** ‚Äì GLiNER-based concept extraction with relation/verb detection

### API Server (`api_server/`)
- `main.py` ‚Äì FastAPI service with multiple endpoints:
  - `/chat` ‚Äì conversational interface
  - `/search` ‚Äì semantic search for keywords
  - `/viz-data` ‚Äì 2D projection data for visualization
  - `/kbs` ‚Äì knowledge base management (list, upload, delete)
- `static/viz.html` ‚Äì D3.js concept visualization
- `static/search.html` ‚Äì standalone semantic search UI

### Command-Line Tools
- `chat_cli.py` ‚Äì command-line chat demo with cross-session context
- `web_ui.py` ‚Äì **Streamlit interface** with semantic search

### Demos & Scripts
- `demo.py`, `demo_clip.py`, `demo_img2img.py` ‚Äì runnable examples
- `scripts/seed_relativity.py` ‚Äì generates test KB with Special Relativity concepts

### Tests
- `tests/test_chatbot.py` ‚Äì regression coverage for chat + persistence
- `tests/test_chaos.py` ‚Äì chaos testing
- `test_reconstruction.py` ‚Äì validates knowledge reconstruction from seed keywords

---

## Installation

```bash
git clone https://github.com/loveless2001/hologram.git
cd hologram
python3 -m venv .venv
source .venv/bin/activate
```

### Core dependencies

```bash
pip install numpy faiss-cpu Pillow
```

### Full feature set

For concept decomposition + API + UI:
```bash
pip install fastapi uvicorn streamlit gliner transformers
```

For OpenAI chatbot:
```bash
pip install openai
```

For CLIP embeddings (semantic image search):
```bash
pip install torch torchvision open_clip_torch
```

Development helpers:
```bash
pip install pytest
```

*(Switch to CUDA wheels if you have a GPU.)*

---

## Quickstart

### 1. Python API

```python
from hologram import Hologram

holo = Hologram.init(use_clip=False)  # add use_gravity=False if FAISS is unavailable
holo.glyphs.create("üùû", title="Curvature Anchor")

trace_id = holo.add_text("üùû", "Memory is gravity. Collapse deferred.")
hits = holo.search_text("gravity wells", top_k=3)

for trace, score in hits:
    print(holo.summarize_hit(trace, score))

holo.save("memory_store.json")
```

Reload later with `Hologram.load("memory_store.json", use_clip=False)`.

### 2. Concept Decomposition

```python
from hologram.text_utils import extract_concepts

text = "Special Relativity describes how time dilation occurs near the speed of light."
concepts = extract_concepts(text)
# Returns: ['Special Relativity', 'time dilation', 'speed of light']
```

### 3. Streamlit UI

```bash
# Start API server
uvicorn api_server.main:app --port 8000

# In another terminal, start Streamlit UI
streamlit run web_ui.py
```

Then:
1. Select `relativity.txt` in sidebar
2. Click **üîÑ Load KB**
3. Use **üîç Semantic Search** tab to search for keywords
4. Or use **üí¨ Chat** tab for conversational queries

### 4. Visualization

Visit `http://localhost:8000/viz/viz.html` after loading a KB to see the 2D concept projection.

---

## API Endpoints

### Knowledge Base Management
- `GET /kbs` ‚Äì list available knowledge bases
- `POST /kbs/upload` ‚Äì upload new KB (text file)
- `DELETE /kbs/{name}` ‚Äì delete KB

### Memory Operations
- `POST /chat` ‚Äì conversational interface (loads KB if `kb_name` provided)
  ```json
  {"message": "What is time dilation?", "kb_name": "relativity.txt"}
  ```

- `POST /search` ‚Äì semantic search
  ```json
  {"query": "speed of light", "top_k": 10}
  ```
  Returns:
  ```json
  {
    "query": "speed of light",
    "results": [
      {"content": "speed of light", "score": 1.0},
      {"content": "spacetime", "score": 0.74}
    ]
  }
  ```

### Visualization
- `GET /viz-data` ‚Äì returns 2D projection points and labels
  ```json
  {
    "points": [[x1, y1], [x2, y2], ...],
    "labels": ["concept1", "concept2", ...]
  }
  ```

---

## Knowledge Base Format

### Text Files (`.txt`)
Place in `data/kbs/`. Each line is processed as follows:
1. **GLiNER extraction**: Sentence ‚Üí atomic concepts
2. **Memory storage**: Each concept added to holographic memory
3. **Gravity field**: Concepts positioned in vector space

Example (`data/kbs/relativity.txt`):
```
Special Relativity describes how time dilation occurs near the speed of light.
Length contraction is observed when an object moves at high velocity.
Mass-energy equivalence states that energy equals mass times the speed of light squared.
```

### JSON Files (`.json`)
Full memory snapshots with gravity state. Generated via:
```python
memory.save("relativity_kb.json")
```

---

## Demos

- Text only: `python demos/demo.py`
- Negation-aware gravity: `python demos/demo_negation.py`
- Reinforcement-based decay: `python demos/demo_decay.py`
- Text ‚Üí image: `python demos/demo_clip.py`
- Image ‚Üí image: `python demos/demo_img2img.py`
- **Knowledge reconstruction**: `python tests/test_reconstruction.py`

---

## Chat CLI

```bash
python chat_cli.py --session alice --memory memory_store.json
```

- Stores each turn in holographic memory and appends JSONL logs under `chatlogs/`
- Replays `--session-window` recent turns and pulls cross-session memories via semantic search
- Uses `OPENAI_API_KEY` by default; falls back to an echo provider when the key or `openai` lib is missing
- Add `--use-clip` to run the chat with CLIP encoders

---

## Tests

```bash
pip install pytest
pytest
```

Tests cover:
- Chat session logging
- In-memory store operations
- JSON persistence
- Chaos testing for visualization

---

## Repository Layout

```
hologram/            Core package
  ‚îú‚îÄ‚îÄ api.py         Hologram API (add_text, search, persistence)
  ‚îú‚îÄ‚îÄ chatbot.py     Chat memory and provider abstractions
  ‚îú‚îÄ‚îÄ gravity.py     Gravity field simulation (concept drift)
  ‚îú‚îÄ‚îÄ embeddings.py  Text/image encoders (hash + CLIP)
  ‚îî‚îÄ‚îÄ text_utils.py  GLiNER-based concept extraction

api_server/          FastAPI service
  ‚îú‚îÄ‚îÄ main.py        REST API (/search, /chat, /viz-data, /kbs)
  ‚îî‚îÄ‚îÄ static/        
      ‚îú‚îÄ‚îÄ viz.html   D3.js concept visualization
      ‚îî‚îÄ‚îÄ search.html Standalone search interface

demos/               Demonstration scripts
  ‚îú‚îÄ‚îÄ demo.py        Text-only walkthrough
  ‚îú‚îÄ‚îÄ demo_clip.py   Text ‚Üí image search
  ‚îú‚îÄ‚îÄ demo_img2img.py Image ‚Üí image similarity
  ‚îú‚îÄ‚îÄ demo_negation.py Negation-aware gravity
  ‚îú‚îÄ‚îÄ demo_decay.py  Reinforcement-based decay
  ‚îú‚îÄ‚îÄ demo_knowledge_base.py KB construction
  ‚îî‚îÄ‚îÄ demo_kg_comparison.py KG comparison

tests/               Test suite
  ‚îú‚îÄ‚îÄ test_chatbot.py Chat memory tests (pytest)
  ‚îú‚îÄ‚îÄ test_chaos.py  Chaos testing
  ‚îú‚îÄ‚îÄ test_api.py    API endpoint validation
  ‚îú‚îÄ‚îÄ test_gliner.py GLiNER extraction tests
  ‚îú‚îÄ‚îÄ test_reconstruction.py Knowledge reconstruction
  ‚îú‚îÄ‚îÄ test_search_relations.py Relation extraction tests
  ‚îî‚îÄ‚îÄ benchmark.py   Performance benchmarks

scripts/             Utility scripts
  ‚îî‚îÄ‚îÄ seed_relativity.py Generate test KB

data/                Data files
  ‚îú‚îÄ‚îÄ kbs/           Knowledge base text files
  ‚îú‚îÄ‚îÄ cat.png        Sample images (for CLIP demos)
  ‚îî‚îÄ‚îÄ dog.png

docs/                Documentation

Root scripts:
  ‚îú‚îÄ‚îÄ chat_cli.py    Command-line chat interface
  ‚îú‚îÄ‚îÄ web_ui.py      Streamlit UI (chat + search)
  ‚îî‚îÄ‚îÄ run_ui.sh      Quick launch script
```

---

## How It Works

### Concept Decomposition Pipeline
1. **Input**: Full sentence (e.g., "Time dilation occurs near the speed of light")
2. **GLiNER**: Extracts entities with labels `[concept, entity, phenomenon, action, ...]`
3. **Order Preservation**: Sorts by appearance to maintain Subject‚ÜíVerb‚ÜíObject flow
4. **Output**: `['Time dilation', 'occurs', 'speed of light']`

### Knowledge Reconstruction
1. **Seed keyword**: User provides (e.g., "speed of light")
2. **Vector search**: Find top-k nearest neighbors in embedding space
3. **Graph Extraction**: Retrieve the connected subgraph (neighbors + relation strengths)
4. **LLM Synthesis**: The LLM receives the graph JSON and reconstructs the narrative
5. **Result**: Coherent explanation derived strictly from memory structure

Example:
```
Seed: "speed of light"
‚Üí Retrieved Graph: 
  - speed of light (mass: 1.2) -> related_to: [time dilation (0.8), energy (0.7)]
  - time dilation (mass: 1.0) -> related_to: [speed of light (0.8), proper time (0.6)]
‚Üí LLM Output: "The speed of light is intrinsically linked to time dilation..."
```

### Contextual Disambiguation (Mitosis)
1. **Detection**: System monitors concepts for "semantic tension" (distinct neighbor clusters).
2. **Trigger**: If clusters diverge beyond a threshold, **Mitosis** occurs.
3. **Split**: `Field` -> `Field_1` (Physics context), `Field_2` (Agri context).
4. **Bridge**: A weak link connects them, allowing "wormhole" jumps for creative associations.

---

## Troubleshooting

### Installation Issues
- `ModuleNotFoundError: faiss`: install `faiss-cpu` (or GPU variant) or call `Hologram.init(use_gravity=False)`
- `RuntimeError: open_clip`: only enable CLIP (`--use-clip`, `use_clip=True`) when `torch` + `open_clip_torch` + `pillow` are installed
- Missing sample images: add your own `cat.png` / `dog.png` (or tweak the demos to point at your data)

### GLiNER Issues
- **Slow loading**: Model downloads on first use (~600MB). Cached for subsequent runs.
- **KeyError during extraction**: Ensure labels list is deduplicated (handled in `text_utils.py`)
- **Missing verbs**: Some abstract verbs may not be captured. Adjust threshold (default: 0.25) in `extract_concepts()`

### API Server
- **404 on /search**: Restart server after adding endpoint
- **"No KB loaded" error**: Use `/chat` with `{"message": "load", "kb_name": "relativity.txt"}` first
- **Empty visualization**: Ensure KB is loaded and contains processed concepts

---

## Performance Notes

- **GLiNER model**: ~600MB download, CPU-efficient for inference
- **Vector index**: FAISS with GPU acceleration if available
- **Concept extraction**: ~0.5-2s per sentence on CPU (cached model)
- **Search latency**: <100ms for top-10 semantic search
- **Visualization**: PCA projection computed on-demand, refreshes every 5s

---

## License

MIT
